---
layout: page
title: Interpretable Evaluation of Language Model Training Trajectories
description: Can we automatically discover what is learnt at various stages of training that influences model behaviour?
img: assets/img/ai_probe.jpg
importance: 1
category: academic
---

Everyone has hypotheses of what is being learnt at various stages of LLM training. People design tests to prove or disprove said hypothesis. But is there a way to automatically discover fine-grained "concepts" being learnt at various stages of training without fine-grained hypotheses? What does this say about the training data? Can we do something about tokens seen at a step to control behavior?
