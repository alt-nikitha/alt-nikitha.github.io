---
---

@string{aps = {American Physical Society,}}


@inproceedings{shahid-et-al-poster2,
  title={Contextual Alchemy: A Framework for Enhanced Readability through Cross-Domain Entity Alignment},
  author="Srikanth, Nikitha  and
      Shahid, Simra  and
      Jandial, Surgan  and
      Krishnamurthy, Balaji",
  abstract={Prior to the development of Large Language Models (LLMs), the pursuit of creative writing or content adjustment mainly focused on tailoring tonality, style, and lexicon to suit reader preferences. In addition, there have been frameworks aimed at simplification like 'Explain it to me like I'm five' and targeted explanation like 'Explain to me like I'm a scientist'.In this work, we present Contextual Alchemy, a framework that identifies examples and its context in a document and suggests alternate examples for different topic of interest, time, and region. Consider that you are reading a document that mentions Magnavox Odyssey. Such an example does not resonate with all readers and they might lose relevance over time. Our framework aims to retrieve other replacable entities in similar context, for example, in the sports domain Reebok has faced a similar outcome to Magnavox Odyssey. In this manner, our work utilises LLMs to enhance readability by adapting entities and context within a document to align closely with varied reader interests, ensuring reading is more engaging, relatable, and factually consistent for diverse readers.},
  year={2023},
  month={Dec},
  booktitle = "NeurIPS 2023 Workshop on Machine Learning for Creativity and Design",
  publisher=NeurIPS,
  url={https://neurips.cc/virtual/2023/75073},
  html={https://sites.google.com/view/reading-reimagined/},
  dimensions={true},
  selected={true},
  abbr = "poster"
}

@inproceedings{shahid-et-al-poster,
  title={Interpretable & Hierarchical Topic Models using Hyperbolic Geometry},
  author="Shahid, Simra  and
      Anand, Tanay  and
      Srikanth, Nikitha  and
      Bhatia, Sumit  and
      Krishnamurthy, Balaji  and
      Puri, Nikaash",
  abstract={Topic Models (TM) are statistical models to learn latent topics present in a collection of text documents. These topics are usually not independent and represent concepts that are related hierarchically. Flat TMs such as LDA fail to capture this inherent hierarchy. To overcome these limitations, Hierarchical Topic Model (HTM) have been proposed that discover latent topics while preserving the inherent hierarchical structure between different topics (for example, aspect hierarchies in reviews and research topic hierarchies in academic repositories). Despite showing great promise, the state-of-the-art HTMs fail to capture coherent hierarchical structures. Also the number of topics in each level is usually unknown and is determined empirically, wasting a lot of time and resources. Finally, HTMs have very long training time, making them unsuitable for real-time production environments. Thus, there is a need for HTMs that (i) offers good hierarchical structures and more meaningful and interpretable topics and that (ii) can automatically find the number of topics at each level without multiple training iterations. In our work, we address the problems mentioned above by utilizing properties of hyperbolic geometry that has been successfully applied in learning hierarchical structures such as ontologies in Knowledge bases, and latent hierarchy between words. Our initial experiments have yielded promising results where the training time has reduced from weeks to less than an hour, and the quantitative metrics have improved with significantly better hierarchical structures. We have attached an abstract with a brief overview of our approach and results.},
  year={2021},
  month={Dec},
  booktitle = "NeurIPS 2021 WiML Workshop 1",
  publisher=NeurIPS,
  url={https://slideslive.com/38970853/interpretable-hierarchical-topic-models-using-hyperbolic-geometry},
  html={https://nips.cc/virtual/2021/33495},
  dimensions={true},
  selected={true},
  abbr = "poster"
}


@inproceedings{shahid-etal-2023-hyhtm,
    title = "{H}y{HTM}: Hyperbolic Geometry-based Hierarchical Topic Model",
    author = "Srikanth, Nikitha  and
      Shahid, Simra  and
      Anand, Tanay  and
      Bhatia, Sumit  and
      Krishnamurthy, Balaji  and
      Puri, Nikaash",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.742",
    doi = "10.18653/v1/2023.findings-acl.742",
    pages = "11672--11688",
    abstract = "Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lower-level topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry-based Hierarchical Topic Model - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to explicitly model hierarchies in topic models. Experimental results with four baselines show that HyHTM can better attend to parent-child relationships among topics. HyHTM produces coherent topic hierarchies that specialize in granularity from generic higher-level topics to specific lower-level topics. Further, our model is significantly faster and leaves a much smaller memory footprint than our best-performing baseline. We have made the source code for our algorithm publicly accessible.",
    abbr = "paper"
}

@patent{Nikitha2024,
  author       = "Srikanth, Nikitha  and
      Shahid, Simra  and
      Anand, Tanay  and
      Bhatia, Sumit  and
      Krishnamurthy, Balaji  and
      Puri, Nikaash",
  title        = {Hierarchical topic model with an interpretable topic hierarchy},
  number       = {US11960520B2},
  year         = {2024},
  month        = {april},
  type         = {U.S. Patent},
  institution  = {U.S. Patent and Trademark Office},
  url          = "https://patents.google.com/patent/US11960520B2/en",
  html = "https://patents.google.com/patent/US11960520B2/en",
  abbr = "patent"
}

